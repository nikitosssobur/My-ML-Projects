{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnQhTv0L2RgLTRgpe+qEUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikitosssobur/My-ML-Projects/blob/main/vgg_16_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIq3Hx_RtAUY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project task: build VGG-16 model for classification of the images from CIFAR-100 dataset to 100 classes.**"
      ],
      "metadata": {
        "id": "tRr4oQZ2geJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Device info\n",
        "print(torch.cuda.is_available())\n",
        "#drive.mount('/content/drive')\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "J7UUh1XMtXQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8373f53f-58b1-4399-8213-d799216d6625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "0\n",
            "Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dictionary = pickle.load(fo, encoding='bytes')\n",
        "        return dictionary"
      ],
      "metadata": {
        "id": "9w42Lb05tXiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unpack the data\n",
        "data_dict = unpickle('/content/sample_data/cifar_data/train')\n",
        "test_data_dict = unpickle('/content/sample_data/cifar_data/test')\n",
        "meta_data_dict = unpickle('/content/sample_data/cifar_data/meta')\n",
        "print(data_dict.keys())\n",
        "print(f'Keys of the meta data dictionary: {meta_data_dict.keys()}')\n",
        "print(f'Number of classes label names: {len(meta_data_dict[b\"fine_label_names\"])}')\n",
        "print(f'Class label names: {meta_data_dict[b\"fine_label_names\"]}')\n",
        "print(f'Number of superclasses label names: {len(meta_data_dict[b\"coarse_label_names\"])}')\n",
        "print(f'Class label names: {meta_data_dict[b\"coarse_label_names\"]}')\n"
      ],
      "metadata": {
        "id": "UniuAVhgtXuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1557501-9350-4511-e1df-e94c62040922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])\n",
            "Keys of the meta data dictionary: dict_keys([b'fine_label_names', b'coarse_label_names'])\n",
            "Number of classes label names: 100\n",
            "Class label names: [b'apple', b'aquarium_fish', b'baby', b'bear', b'beaver', b'bed', b'bee', b'beetle', b'bicycle', b'bottle', b'bowl', b'boy', b'bridge', b'bus', b'butterfly', b'camel', b'can', b'castle', b'caterpillar', b'cattle', b'chair', b'chimpanzee', b'clock', b'cloud', b'cockroach', b'couch', b'crab', b'crocodile', b'cup', b'dinosaur', b'dolphin', b'elephant', b'flatfish', b'forest', b'fox', b'girl', b'hamster', b'house', b'kangaroo', b'keyboard', b'lamp', b'lawn_mower', b'leopard', b'lion', b'lizard', b'lobster', b'man', b'maple_tree', b'motorcycle', b'mountain', b'mouse', b'mushroom', b'oak_tree', b'orange', b'orchid', b'otter', b'palm_tree', b'pear', b'pickup_truck', b'pine_tree', b'plain', b'plate', b'poppy', b'porcupine', b'possum', b'rabbit', b'raccoon', b'ray', b'road', b'rocket', b'rose', b'sea', b'seal', b'shark', b'shrew', b'skunk', b'skyscraper', b'snail', b'snake', b'spider', b'squirrel', b'streetcar', b'sunflower', b'sweet_pepper', b'table', b'tank', b'telephone', b'television', b'tiger', b'tractor', b'train', b'trout', b'tulip', b'turtle', b'wardrobe', b'whale', b'willow_tree', b'wolf', b'woman', b'worm']\n",
            "Number of superclasses label names: 20\n",
            "Class label names: [b'aquatic_mammals', b'fish', b'flowers', b'food_containers', b'fruit_and_vegetables', b'household_electrical_devices', b'household_furniture', b'insects', b'large_carnivores', b'large_man-made_outdoor_things', b'large_natural_outdoor_scenes', b'large_omnivores_and_herbivores', b'medium_mammals', b'non-insect_invertebrates', b'people', b'reptiles', b'small_mammals', b'trees', b'vehicles_1', b'vehicles_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(data_dict[b'fine_labels']), max(data_dict[b'fine_labels']))\n",
        "classes_labels = np.arange(0, 100)\n",
        "classes_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8haEp44QMlH2",
        "outputId": "237894b3-418d-4dd5-8867-303f8e885b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 99\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_labels_by_classes(data_list, labels):\n",
        "    data = np.array(data_list)\n",
        "    labels_dict = {label: np.where(data == label)[0] for label in labels}\n",
        "    return labels_dict\n",
        "\n",
        "#labels_dict = divide_labels_by_classes(data_dict[b'fine_labels'], classes_labels)\n",
        "#labels_dict[0]\n",
        "train_labels_dict = divide_labels_by_classes(data_dict[b'fine_labels'], classes_labels)\n",
        "test_labels_dict = divide_labels_by_classes(test_data_dict[b'fine_labels'], classes_labels)\n",
        "\n",
        "'''\n",
        "Создай список и посмотри сколько сколько картинок получается по каждому классу\n",
        "'''\n",
        "num_of_images_per_class_train = [len(train_labels_dict[label]) for label in train_labels_dict]\n",
        "num_of_images_per_class_test = [len(test_labels_dict[label]) for label in test_labels_dict]\n",
        "print(num_of_images_per_class_train)\n",
        "print(num_of_images_per_class_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNjlrtQypBw-",
        "outputId": "1728128d-8b26-4a71-be4c-9499e67cb7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n",
            "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing pipeline\n",
        "\n",
        "#Function returns numerical data of fixed image for choosen channel (red, green or blue)\n",
        "def get_channel_data(data_dict, img_number, channel):\n",
        "    if channel == 'red':  return data_dict[b'data'][img_number][: 1024]\n",
        "    elif channel == 'blue':   return data_dict[b'data'][img_number][1024 : 2048]\n",
        "    else: return data_dict[b'data'][img_number][2048: ]\n",
        "\n",
        "\n",
        "#Function returns reshaped image (32 x 32)\n",
        "def reshape_img(channel_data, width, height):\n",
        "    return np.reshape(channel_data, (width, height))\n",
        "\n",
        "\n",
        "#Function returns preprocessed image by taken number\n",
        "def image_preprocessing(data_dict, img_number):\n",
        "    red_channel = get_channel_data(data_dict, img_number, 'red')\n",
        "    blue_channel = get_channel_data(data_dict, img_number, 'blue')\n",
        "    green_channel = get_channel_data(data_dict, img_number, 'green')\n",
        "\n",
        "    rc_matrix = reshape_img(red_channel, 32, 32)\n",
        "    bc_matrix = reshape_img(blue_channel, 32, 32)\n",
        "    gc_matrix = reshape_img(green_channel, 32, 32)\n",
        "\n",
        "    red_img = Image.fromarray(rc_matrix, mode=\"L\")\n",
        "    green_img = Image.fromarray(gc_matrix, mode=\"L\")\n",
        "    blue_img = Image.fromarray(bc_matrix, mode=\"L\")\n",
        "\n",
        "    width, height = red_img.size\n",
        "    result_img = Image.new('RGB', (width, height))\n",
        "    result_img.putdata(list(zip(red_img.getdata(), green_img.getdata(), blue_img.getdata())))\n",
        "    return result_img\n",
        "\n",
        "\n",
        "def create_sample(data_dict):\n",
        "    new_data = [image_preprocessing(data_dict, img_number) for img_number in range(len(data_dict[b'data']))]\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def create_batch(data_dict, batch_size):\n",
        "    labels_dict = divide_labels_by_classes(data_dict[b'fine_labels'], classes_labels)\n",
        "    labels_num = len(labels_dict)\n",
        "    images_per_class_num, residual_images_num = batch_size // labels_num, batch_size % labels_num\n",
        "    if images_per_class_num > 0:\n",
        "        for label in iter(labels_dict):\n",
        "            if label == 0:\n",
        "                random_indices = np.random.choice(labels_dict[label], size = images_per_class_num)\n",
        "            else:\n",
        "                random_label_indices = np.random.choice(labels_dict[label], size = images_per_class_num)\n",
        "                random_indices = np.concatenate((random_indices, random_label_indices))\n",
        "\n",
        "        if residual_images_num != 0:\n",
        "            residual_labels = ([np.random.randint(0, labels_num) for i in range(residual_images_num)])\n",
        "            residual_random_indices = np.array([np.random.choice(labels_dict[label]) for labels in residual_labels])\n",
        "            random_indices = np.concatenate((random_indices, residual_random_indices))\n",
        "\n",
        "    else:\n",
        "        random_indices = np.random.choice(data_dict[b'fine_labels'], size = batch_size)\n",
        "\n",
        "    x_data = [image_preprocessing(data_dict, img_num) for img_num in iter(random_indices)]\n",
        "    y_data = torch.tensor([data_dict[b'fine_labels'][i] for i in iter(random_indices)], dtype = torch.long, device = device)\n",
        "    #return [x_data[i] for i in random_indices], y_data[random_indices]\n",
        "    return x_data, y_data\n",
        "\n",
        "\n",
        "def get_unique_indices_by_batches(labels_data, labels_num, epoch_num, batch_size, shuffle_mode = True):\n",
        "    '''\n",
        "    This function excludes already used indices and returns unique batches of images by epoch_num.\n",
        "    labels_data: target label for each input image of the data_dict.\n",
        "    labels_num: general number of classes.\n",
        "    indices: indices of all images in the train sample\n",
        "    labels: indices of labels of the classes\n",
        "    batches_dict: output dictionary in the following format:\n",
        "    'epoch_number'(key): list of indices of random choosen unique images by all classes size of batch_size (value)\n",
        "    images_num_per_class: integer number of images in batch taken from each class\n",
        "    residual_images_num: residual images in the case when batch_size % labels_num != 0\n",
        "    images_num: total number of images from train data which is used in the process of training (without residual images!!)\n",
        "    residual_images_num: number of residue images after integer division from batch size by labels_num\n",
        "    '''\n",
        "    indices, labels, batches_dict = np.arange(len(labels_data)), np.arange(labels_num), {}\n",
        "    images_num_per_class, residual_images_num = batch_size // labels_num, batch_size % labels_num\n",
        "    images_num = images_num_per_class * epoch_num\n",
        "    indices_per_class_dict = divide_labels_by_classes(labels_data, labels)\n",
        "    min_number_of_images_per_class = min(tuple([len(indices_per_class_dict[label]) for label in labels]))\n",
        "    if images_num < min_number_of_images_per_class:\n",
        "        for epoch in range(epoch_num):\n",
        "            if images_num_per_class > 0:\n",
        "                for label in labels:\n",
        "                    if label == 0:\n",
        "                        random_indices = np.random.choice(indices_per_class_dict[0], size = images_num_per_class)\n",
        "                        indices_per_class_dict[0] = np.setdiff1d(indices_per_class_dict[0], random_indices)\n",
        "                    else:\n",
        "                        random_label_indices = np.random.choice(indices_per_class_dict[label], size = images_num_per_class)\n",
        "                        indices_per_class_dict[label] = np.setdiff1d(indices_per_class_dict[label], random_label_indices)\n",
        "                        random_indices = np.concatenate((random_indices, random_label_indices))\n",
        "\n",
        "                if residual_images_num != 0:\n",
        "                    residual_labels = np.random.choice(labels, size = residual_images_num)\n",
        "                    #residual_random_indices = np.array([np.random.choice(indices_per_class_dict[label]) for label in residual_labels])\n",
        "                    for label in residual_labels:\n",
        "                        random_residual_label = np.array([np.random.choice(indices_per_class_dict[label])])\n",
        "                        indices_per_class_dict[label] = np.setdiff1d(indices_per_class_dict[label], random_residual_label)\n",
        "                        random_indices = np.concatenate((random_indices, random_residual_label))\n",
        "\n",
        "            else:\n",
        "                random_indices = np.random.choice(indices, size = batch_size)\n",
        "                indices = np.setdiff1d(indices, random_indices)\n",
        "\n",
        "\n",
        "            if shuffle_mode is True:  np.random.shuffle(random_indices)\n",
        "\n",
        "            batches_dict[epoch] = random_indices\n",
        "    return batches_dict\n",
        "\n",
        "\n",
        "\n",
        "def batch_from_indices(data_dict, indices_list):\n",
        "    x_data = [image_preprocessing(data_dict, img_num) for img_num in iter(indices_list)]\n",
        "    y_data = torch.tensor([data_dict[b'fine_labels'][i] for i in iter(indices_list)], dtype = torch.long, device = device)\n",
        "    return x_data, y_data\n",
        "\n"
      ],
      "metadata": {
        "id": "vLR615XmtX6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_by_batches = get_unique_indices_by_batches(data_dict[b'fine_labels'], 100, 10, 400)\n",
        "x_batch_data, y_batch_data = batch_from_indices(data_dict, indices_by_batches[0])\n",
        "#get_unique_indices_by_batches"
      ],
      "metadata": {
        "id": "6bXeZbFJdl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image example\n",
        "processed_image = image_preprocessing(data_dict, 10)\n",
        "print(f'Type of the image: {type(processed_image)}')\n",
        "np_image_repres = np.array(processed_image)\n",
        "torch_image_repres = transforms.functional.pil_to_tensor(processed_image)\n",
        "print(f'Shape of the image in np repres: {np.shape(np_image_repres)}')\n",
        "print(f'Form of the image in numpy representation: \\n {np_image_repres}')\n",
        "processed_image\n"
      ],
      "metadata": {
        "id": "WB_hkztYtYOV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "990ec9c5-958d-4f42-ac20-1876896e4b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the image: <class 'PIL.Image.Image'>\n",
            "Shape of the image in np repres: (32, 32, 3)\n",
            "Form of the image in numpy representation: \n",
            " [[[ 83 178 132]\n",
            "  [ 67 160 114]\n",
            "  [ 61 154 108]\n",
            "  ...\n",
            "  [  9  57  26]\n",
            "  [ 16  66  34]\n",
            "  [ 35  94  59]]\n",
            "\n",
            " [[ 75 173 124]\n",
            "  [ 71 167 119]\n",
            "  [ 55 152 103]\n",
            "  ...\n",
            "  [ 39 107  67]\n",
            "  [  9  65  28]\n",
            "  [ 12  66  31]]\n",
            "\n",
            " [[ 73 173 121]\n",
            "  [ 68 166 115]\n",
            "  [ 74 172 122]\n",
            "  ...\n",
            "  [ 91 178 132]\n",
            "  [ 53 125  83]\n",
            "  [ 14  70  34]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[159 223 187]\n",
            "  [150 159 146]\n",
            "  [191 143 154]\n",
            "  ...\n",
            "  [152 136 131]\n",
            "  [173 158 143]\n",
            "  [122 150 123]]\n",
            "\n",
            " [[157 202 171]\n",
            "  [155 132 133]\n",
            "  [183 141 152]\n",
            "  ...\n",
            "  [213 159 168]\n",
            "  [196 153 146]\n",
            "  [169 180 157]]\n",
            "\n",
            " [[150 186 160]\n",
            "  [155 122 127]\n",
            "  [179 139 149]\n",
            "  ...\n",
            "  [212 148 159]\n",
            "  [187 135 136]\n",
            "  [148 154 135]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Differences between representations\n",
        "print(f'Shape of the image in torch representation: {torch_image_repres.size()}')\n",
        "print(f'Form of the image in torch representation: \\n {torch_image_repres}')\n",
        "print(f'{torch_image_repres[0]}, \\n, {torch_image_repres[0].size()}')"
      ],
      "metadata": {
        "id": "cD27MLcGvHQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345ff310-2c5a-4b7d-a47b-8aeeb6f52e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the image in torch representation: torch.Size([3, 32, 32])\n",
            "Form of the image in torch representation: \n",
            " tensor([[[ 83,  67,  61,  ...,   9,  16,  35],\n",
            "         [ 75,  71,  55,  ...,  39,   9,  12],\n",
            "         [ 73,  68,  74,  ...,  91,  53,  14],\n",
            "         ...,\n",
            "         [159, 150, 191,  ..., 152, 173, 122],\n",
            "         [157, 155, 183,  ..., 213, 196, 169],\n",
            "         [150, 155, 179,  ..., 212, 187, 148]],\n",
            "\n",
            "        [[178, 160, 154,  ...,  57,  66,  94],\n",
            "         [173, 167, 152,  ..., 107,  65,  66],\n",
            "         [173, 166, 172,  ..., 178, 125,  70],\n",
            "         ...,\n",
            "         [223, 159, 143,  ..., 136, 158, 150],\n",
            "         [202, 132, 141,  ..., 159, 153, 180],\n",
            "         [186, 122, 139,  ..., 148, 135, 154]],\n",
            "\n",
            "        [[132, 114, 108,  ...,  26,  34,  59],\n",
            "         [124, 119, 103,  ...,  67,  28,  31],\n",
            "         [121, 115, 122,  ..., 132,  83,  34],\n",
            "         ...,\n",
            "         [187, 146, 154,  ..., 131, 143, 123],\n",
            "         [171, 133, 152,  ..., 168, 146, 157],\n",
            "         [160, 127, 149,  ..., 159, 136, 135]]], dtype=torch.uint8)\n",
            "tensor([[ 83,  67,  61,  ...,   9,  16,  35],\n",
            "        [ 75,  71,  55,  ...,  39,   9,  12],\n",
            "        [ 73,  68,  74,  ...,  91,  53,  14],\n",
            "        ...,\n",
            "        [159, 150, 191,  ..., 152, 173, 122],\n",
            "        [157, 155, 183,  ..., 213, 196, 169],\n",
            "        [150, 155, 179,  ..., 212, 187, 148]], dtype=torch.uint8), \n",
            ", torch.Size([32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deeply understanding what is the format of image\n",
        "print(np_image_repres[0], '\\n', np.shape(np_image_repres[0]))\n",
        "print((np.min(np_image_repres), np.max(np_image_repres)))"
      ],
      "metadata": {
        "id": "AtANi4B1nJFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baabb984-dd7c-4b99-a5a7-45c03739b468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 83 178 132]\n",
            " [ 67 160 114]\n",
            " [ 61 154 108]\n",
            " [ 58 155 108]\n",
            " [ 66 165 117]\n",
            " [ 72 171 124]\n",
            " [ 59 158 111]\n",
            " [ 50 151 102]\n",
            " [ 64 167 114]\n",
            " [ 63 166 114]\n",
            " [ 62 165 113]\n",
            " [ 61 165 112]\n",
            " [ 53 160 104]\n",
            " [ 42 149  92]\n",
            " [ 57 164 107]\n",
            " [ 62 169 113]\n",
            " [ 73 171 120]\n",
            " [121 212 165]\n",
            " [137 227 181]\n",
            " [131 222 175]\n",
            " [127 214 171]\n",
            " [112 191 156]\n",
            " [ 72 148 113]\n",
            " [ 64 136 101]\n",
            " [ 94 165 130]\n",
            " [106 187 147]\n",
            " [ 98 177 138]\n",
            " [ 73 141 106]\n",
            " [ 29  85  54]\n",
            " [  9  57  26]\n",
            " [ 16  66  34]\n",
            " [ 35  94  59]] \n",
            " (32, 3)\n",
            "(5, 255)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = image_preprocessing(test_data_dict, 10)\n",
        "test_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "V7ZsIckSyxrc",
        "outputId": "4114a83e-6e79-4db2-b095-412ffa547b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating random image in the shape of images from dataset for testing our model\n",
        "some_data = torch.randint(0, 255, (3, 32, 32), dtype = torch.uint8)\n",
        "some_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PW39vBZAMS8",
        "outputId": "ed10e3c5-65ad-447d-c4ca-f11d2699109e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 55, 121, 214,  ..., 132,  16,  57],\n",
              "         [213, 201, 100,  ...,  46, 174,  12],\n",
              "         [ 95, 159, 144,  ..., 140, 233, 213],\n",
              "         ...,\n",
              "         [ 70, 139, 125,  ..., 113, 197,  26],\n",
              "         [ 61, 162, 119,  ..., 187, 165, 214],\n",
              "         [ 91,  17, 185,  ..., 122,  61, 220]],\n",
              "\n",
              "        [[ 78, 161, 252,  ..., 158, 241, 119],\n",
              "         [200,   2,  49,  ..., 246, 168,  29],\n",
              "         [129, 199,   0,  ...,   9,  89, 150],\n",
              "         ...,\n",
              "         [ 79, 170,  33,  ...,  76, 235, 201],\n",
              "         [123, 184, 189,  ..., 238, 163, 137],\n",
              "         [ 56,  54, 222,  ..., 166, 175, 249]],\n",
              "\n",
              "        [[157, 152,   6,  ..., 128, 124, 106],\n",
              "         [220, 161,  95,  ...,  95, 133,  85],\n",
              "         [108, 133,  92,  ..., 228,  10,  94],\n",
              "         ...,\n",
              "         [ 72,   3, 225,  ..., 116, 103, 244],\n",
              "         [232,  14,  58,  ...,  65,  29, 201],\n",
              "         [125, 238, 239,  ..., 138,  27, 167]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating models**\n"
      ],
      "metadata": {
        "id": "TmXjRQ37Lnkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating VGG-16 model\n",
        "\n",
        "vgg_transforms = transforms.Compose([transforms.Resize(224), #resizing image to 224 x 224\n",
        "        transforms.ToTensor(), #changing format of image from PIL Image to torch.Tensor\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) #normalizing pixels of input image)\n",
        "\n",
        "\n",
        "class ConvNetVGG(nn.Module):\n",
        "    #VGG-16 model implementation\n",
        "    def __init__(self):\n",
        "        #input image shape: 224 x 224 x 3\n",
        "        super().__init__()\n",
        "        self.kernel_size, self.padding, self.pool_kernel_size = (3, 3), (1, 1), (2, 2)\n",
        "        self.classes = 100\n",
        "        channels = (3, 64, 128, 256, 512)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv11 = nn.Conv2d(channels[0], channels[1], self.kernel_size, padding = self.padding) #input channels: 3  output channels: 64\n",
        "        self.conv12 = nn.Conv2d(channels[1], channels[1], self.kernel_size, padding = self.padding) #in channels: 64  out channels: 64\n",
        "        self.conv21 = nn.Conv2d(channels[1], channels[2], self.kernel_size, padding = self.padding) #in channels: 64  out channels: 128\n",
        "        self.conv22 = nn.Conv2d(channels[2], channels[2], self.kernel_size, padding = self.padding) #in channels: 128 out channels: 128\n",
        "        self.conv31 = nn.Conv2d(channels[2], channels[3], self.kernel_size, padding = self.padding) #in channels: 128 out channels: 256\n",
        "        self.conv32 = nn.Conv2d(channels[3], channels[3], self.kernel_size, padding = self.padding) #in channels: 256 out channels: 256\n",
        "        self.conv33 = nn.Conv2d(channels[3], channels[4], self.kernel_size, padding = self.padding) #in channels: 256 out channels: 512\n",
        "        self.conv41 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.conv42 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.conv43 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.conv51 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.conv52 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.conv53 = nn.Conv2d(channels[4], channels[4], self.kernel_size, padding = self.padding)\n",
        "        self.maxpool = nn.MaxPool2d(self.pool_kernel_size, stride = (2, 2))\n",
        "        self.flatten = torch.flatten  #reference to pytorch function\n",
        "        self.fc1, self.fc2, self.fc3 = nn.Linear(7 * 7 * 512, 4096), nn.Linear(4096, 4096), nn.Linear(4096, self.classes)\n",
        "        #fully connected layers\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            #if isinstance(m, nn.Linear):\n",
        "            if 'weight' in name:\n",
        "                nn.init.normal_(param.data, mean = 0, std = 0.01)\n",
        "\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param.data)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        tr_x = vgg_transforms(x).to(device = device)  #trasformed x  input: x of size (3, 32, 32)\n",
        "        tr_x = tr_x.half()\n",
        "        x1 = self.relu(self.conv11(tr_x))\n",
        "        x2 = self.maxpool(self.relu(self.conv12(x1)))\n",
        "        x3 = self.relu(self.conv21(x2))\n",
        "        x4 = self.maxpool(self.relu(self.conv22(x3)))\n",
        "        x5 = self.relu(self.conv31(x4))\n",
        "        x6 = self.relu(self.conv32(x5))\n",
        "        x7 = self.maxpool(self.relu(self.conv33(x6)))\n",
        "        x8 = self.relu(self.conv41(x7))\n",
        "        x9 = self.relu(self.conv42(x8))\n",
        "        x10 = self.maxpool(self.relu(self.conv43(x9)))\n",
        "        x11 = self.relu(self.conv51(x10))\n",
        "        x12 = self.relu(self.conv52(x11))\n",
        "        x13 = self.maxpool(self.relu(self.conv53(x12)))\n",
        "        x14 = self.relu(self.dropout(self.fc1(self.flatten(x13))))\n",
        "        x15 = self.relu(self.dropout(self.fc2(x14)))\n",
        "        #x16 = nn.Softmax(self.fc3(x15))\n",
        "        #return x16\n",
        "        return self.fc3(x15)\n"
      ],
      "metadata": {
        "id": "kf60vAdHtYc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9"
      ],
      "metadata": {
        "id": "PTnNUjlFnZVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model initialization\n",
        "vgg16_model = ConvNetVGG()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(vgg16_model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)"
      ],
      "metadata": {
        "id": "-KbEGpUBkCXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters info\n",
        "vgg16_model.to(device)\n",
        "\n",
        "print(\"Before changing weights dtype \\n\")\n",
        "for param in vgg16_model.parameters():\n",
        "    print(param.dtype)\n",
        "\n",
        "for param in vgg16_model.parameters():\n",
        "    param.data = param.data.half()\n",
        "\n",
        "print(\"After changing weights dtype \\n\")\n",
        "for param in vgg16_model.parameters():\n",
        "    print(param.dtype)"
      ],
      "metadata": {
        "id": "MProdS0ydrQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model = ConvNetVGG()\n",
        "#print(vgg16_model.forward(some_data))\n",
        "#print(vgg_transforms(some_data))\n",
        "some_data_pil = transforms.functional.to_pil_image(some_data)\n",
        "#transformed_data = vgg_transforms(some_data_pil)\n",
        "#print(transformed_data)\n",
        "\n",
        "#prep_image = vgg16_model.forward(test_image)\n",
        "#prep_image\n",
        "#some_data_preproc = vgg16_model(some_data_pil)\n",
        "some_data_preproc = vgg16_model.forward(some_data_pil)\n",
        "print(torch.argmax(some_data_preproc.dim))\n",
        "some_data_pil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "JOksej1SAsqp",
        "outputId": "b3320e6c-910f-4767-8ba2-a889d37fb669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input type (c10::Half) and bias type (float) should be the same",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9dc60e848102>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#prep_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#some_data_preproc = vgg16_model(some_data_pil)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msome_data_preproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_data_pil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_data_preproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msome_data_pil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4988130aee27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#trasformed x  input: x of size (3, 32, 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::Half) and bias type (float) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_prediction(model, batch, mode = 'train'):\n",
        "    if mode == 'train':\n",
        "        logits_list = [model(image).requires_grad_() for image in batch]\n",
        "        prediction = torch.stack(logits_list).to(device = device) #prediction: tensor of batch logits\n",
        "    else:\n",
        "        prediction = [torch.argmax(nn.Softmax(model(image)).dim) for image in batch] #prediction: list of the most probable labels\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def random_predict(batch_size):\n",
        "    rand_batch = []\n",
        "    for i in iter(range(batch_size)):\n",
        "        random_tensor = torch.randint(0, 255, (3, 32, 32), dtype = torch.uint8)\n",
        "        random_image_pil = transforms.functional.to_pil_image(random_tensor)\n",
        "        rand_batch.append(random_image_pil)\n",
        "\n",
        "    rand_predict = get_batch_prediction(vgg16_model, rand_batch)\n",
        "    return rand_predict\n",
        "\n",
        "#rand_predict = random_predict(30)\n",
        "#rand_predict"
      ],
      "metadata": {
        "id": "61udqA6vMgtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_u1qceMdAuuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH_NUM = 70\n",
        "train_loss_history, test_loss_history = [], []\n",
        "x_train, y_train = create_sample(data_dict), data_dict[b'fine_labels']\n",
        "x_test, y_test = create_sample(test_data_dict), test_data_dict[b'fine_labels']\n",
        "batch_size = 256"
      ],
      "metadata": {
        "id": "vDqe9LHcCx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_train))"
      ],
      "metadata": {
        "id": "gHOrLkUgD8CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt.zero_grad()\n",
        "x_train_batch, y_train_batch = create_batch(x_train, y_train, 10)\n",
        "y_batch_predicted = get_batch_prediction(vgg16_model, x_train_batch)\n",
        "print(type(y_train_batch), type(y_batch_predicted))\n",
        "print(y_train_batch)\n",
        "print(y_batch_predicted)\n",
        "print(torch.tensor(y_train_batch).size(), y_batch_predicted.size())\n",
        "loss_ = loss_function.forward(y_batch_predicted, torch.tensor(y_train_batch, dtype=torch.float))"
      ],
      "metadata": {
        "id": "Wn3BEUlkC2I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing for training process (setting hyperparameters etc.)\n",
        "EPOCH_NUM = 90\n",
        "batch_size = 280\n",
        "#epsilon = 0.00001\n",
        "indices_by_batches = get_unique_indices_by_batches(data_dict[b'fine_labels'], 100, EPOCH_NUM, batch_size)\n",
        "indices_by_test_batches = get_unique_indices_by_batches(test_data_dict[b'fine_labels'], 100, EPOCH_NUM, 90)\n",
        "train_loss_history, test_loss_history = [], []\n",
        "#epsilon = 0.000001\n",
        "vgg16_model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTzofwAwPDVF",
        "outputId": "d849184c-344a-422c-97fe-616556b75eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNetVGG(\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (conv11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv33): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv41): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv51): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv53): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (fc3): Linear(in_features=4096, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training VGG16 model\n",
        "#EPOCH_NUM = 70\n",
        "#EPOCH_NUM = 170\n",
        "#train_loss_history, test_loss_history = [], []\n",
        "#x_train, y_train = create_sample(data_dict), data_dict[b'fine_labels']\n",
        "#x_test, y_test = create_sample(test_data_dict), test_data_dict[b'fine_labels']\n",
        "#batch_size = 250\n",
        "#batch_size = 250\n",
        "#epsilon = 0.000001\n",
        "\n",
        "#indices_by_batches = get_unique_indices_by_batches(data_dict[b'fine_labels'], 100, EPOCH_NUM, batch_size)\n",
        "#batch_from_indices(data_dict, indices_by_batches[epoch_num])\n",
        "\n",
        "for epoch_num in range(EPOCH_NUM):\n",
        "    opt.zero_grad()\n",
        "    #x_train_batch, y_train_batch = create_batch(x_train, y_train, train_labels_dict, batch_size)\n",
        "    #x_train_batch, y_train_batch = create_batch(data_dict, batch_size)\n",
        "    x_train_batch, y_train_batch = batch_from_indices(data_dict, indices_by_batches[epoch_num])\n",
        "    y_batch_predicted = get_batch_prediction(vgg16_model, x_train_batch)\n",
        "    loss = loss_function(y_batch_predicted, y_train_batch)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    train_loss_history.append(loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #x_test_batch, y_test_batch = create_batch(x_test, y_test, test_labels_dict, 100)\n",
        "        #x_test_batch, y_test_batch = create_batch(test_data_dict, 100)\n",
        "        x_test_batch, y_test_batch = batch_from_indices(test_data_dict, indices_by_test_batches[epoch_num])\n",
        "        y_predicted_test = get_batch_prediction(vgg16_model, x_test_batch)\n",
        "        test_loss_history.append(loss_function(y_predicted_test, y_test_batch).item())\n",
        "\n",
        "    '''\n",
        "    if epoch_num > 1 and abs(train_loss_history[-1] - train_loss_history[-2]) < epsilon:\n",
        "        opt.param_groups[0]['lr'] /= 1.1\n",
        "        #opt.param_groups[0]['weight_decay'] /= 1.5\n",
        "    '''\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.plot(np.arange(len(train_loss_history)), train_loss_history, label='train')\n",
        "    plt.plot(np.arange(len(test_loss_history)), test_loss_history, label='test')\n",
        "    plt.title('CrossEntropy Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f5zRC-bBz94Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "9d76e204-bd6b-4733-f8ef-8ccc0e6002de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-dd805a05fd17>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_batch_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDoGc-Rjp1Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LF6nuYEjBJis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loss_history[-1], test_loss_history[-1])\n",
        "print(train_loss_history[-10:], test_loss_history[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3744lm8FP_Z",
        "outputId": "7d84e95a-724c-44e8-a7c0-38a018522bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.60546875 4.60546875\n",
            "[4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875] [4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875, 4.60546875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(5).to(device = device)"
      ],
      "metadata": {
        "id": "unnTgTyUN2z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "F82jthgKgxd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in vgg16_model.named_parameters():\n",
        "    print(f'Name: {name} \\n', param, '\\n\\n')"
      ],
      "metadata": {
        "id": "_g64hJJs9WOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing model on the test data\n",
        "vgg16_model.eval()\n",
        "x_test_batch, y_test_batch = create_batch(test_data_dict, 400)\n",
        "y_test_batch_predicted = get_batch_prediction(vgg16_model, x_test_batch, mode = 'test')\n",
        "y_test_batch = y_test_batch.cpu().numpy()\n",
        "y_test_batch_predicted = [x.cpu() for x in y_test_batch_predicted]\n",
        "accuracy = accuracy_score(y_test_batch, y_test_batch_predicted)\n",
        "precision = precision_score(y_test_batch, y_test_batch_predicted, average='macro')\n",
        "recall = recall_score(y_test_batch, y_test_batch_predicted, average='macro')\n",
        "\n",
        "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')\n",
        "print(y_test_batch)\n",
        "print(y_test_batch_predicted)"
      ],
      "metadata": {
        "id": "UA8_6efPxHLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}